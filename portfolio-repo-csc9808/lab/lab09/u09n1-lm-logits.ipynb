{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc8Qlh1TEgC"
   },
   "source": [
    "# Logits in Causal Language Models\n",
    "\n",
    "Task: Ask a language model for how likely each token is to be the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_8RWp3TX-8"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start in the same way as the tokenization notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "osKgPaDwhaN4"
   },
   "outputs": [],
   "source": [
    "# If the import fails, uncomment the following line:\n",
    "# !pip install transformers\n",
    "import torch\n",
    "from torch import tensor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One step in this notebook will ask you to write a function. The most common error when function-ifying notebook code is accidentally using a global variable instead of a value computed in the function. This is a quick and dirty little utility to check for that mistake. (For a more polished version, check out [`localscope`](https://localscope.readthedocs.io/en/latest/README.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_global_vars(func, allowed_globals):\n",
    "    import inspect\n",
    "    used_globals = set(inspect.getclosurevars(func).globals.keys())\n",
    "    disallowed_globals = used_globals - set(allowed_globals)\n",
    "    if len(disallowed_globals) > 0:\n",
    "        raise AssertionError(f\"The function {func.__name__} used unexpected global variables: {list(disallowed_globals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNKbIh8hyDg"
   },
   "source": [
    "Download and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IM5o_4w1hfyV"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
    "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-Z9_U0LUEVQ",
    "outputId": "cb7d4eb7-bc54-4583-dd10-e0cb185494da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 strings in its vocabulary.\n",
      "The model has 81,912,576 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tokenizer has {len(tokenizer.get_vocab())} strings in its vocabulary.\")\n",
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOUiz_PsUZgS"
   },
   "source": [
    "## Task\n",
    "\n",
    "In the tokenization notebook, we simply used the `generate` method to have the model generate some text. Now we'll do it ourselves.\n",
    "\n",
    "Consider the following phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JS7Z-DjoUiLK"
   },
   "outputs": [],
   "source": [
    "phrase = \"This weekend I plan to\"\n",
    "# Another one to try later. This was a famous early example of the GPT-2 model:\n",
    "# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Call the `tokenizer` on the phrase to get a `batch` that includes `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QpyeKakrjfpt"
   },
   "outputs": [],
   "source": [
    "batch = tokenizer(phrase, return_tensors='pt')\n",
    "input_ids = batch['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Call the `model` on the `input_ids`. Examine the shape of the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEy1QBTDotjU",
    "outputId": "70a3312b-1bc7-47b0-c1d6-4739c93f56c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: [1, 5, 50257]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # This tells PyTorch we don't need it to compute gradients for us.\n",
    "    model_output = model(input_ids)\n",
    "print(f\"logits shape: {list(model_output.logits.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Pull out the logits corresponding to the *last* token in the input phrase. Hint: Think about what each number in the shape means.\n",
    "\n",
    "Note: The `model` returns a dictionary-like object. The `logits` are in `model_output.logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_logits = model_output.logits[0, -1, :]\n",
    "assert last_token_logits.shape == (len(tokenizer.get_vocab()),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Identify the token id and corresponding string of the most likely next token.\n",
    "\n",
    "To find the most likely token, we need to find the *index* of the *largest value* in the `last_token_logits`. The method that does this is called `argmax`. (It's a common enough operation that it's built into PyTorch.)\n",
    "\n",
    "Note: The `tokenizer` has a `decode` method that takes a token id, or a list of token ids, and returns the corresponding string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the phrase: This weekend I plan to\n",
      "Most likely next token: 467, which corresponds to ' go', with probability 5.98%\n"
     ]
    }
   ],
   "source": [
    "last_token_probabilities = last_token_logits.softmax(dim=-1)\n",
    "# dim=-1 means to compute the softmax over the last dimension\n",
    "\n",
    "# Find the most likely token\n",
    "most_likely_token_id = last_token_probabilities.argmax().item()\n",
    "decoded_token = tokenizer.decode(most_likely_token_id)\n",
    "probability_of_most_likely_token = last_token_probabilities[most_likely_token_id].item()\n",
    "\n",
    "# Print the results\n",
    "print(\"For the phrase:\", phrase)\n",
    "print(f\"Most likely next token: {most_likely_token_id}, which corresponds to {repr(decoded_token)}, with probability {probability_of_most_likely_token:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Use the `topk` method to find the top-10 most likely choices for the next token.\n",
    "\n",
    "See the documentation for [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html). Calling `topk` on a tensor returns a named tuple with two tensors: `values` and `indices`. The `values` are the top-k values, and the `indices` are the indices of those values in the original tensor. (In this case, the indices are the token ids.)\n",
    "\n",
    "*Note*: This uses Pandas to make a nicely displayed table, and a *list comprehension* to decode the tokens. You don't *need* to understand how this all works, but I highly encourage thinking about what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most likely token index from topk is 467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188717/1319892629.py:13: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  most_likely_tokens_df.style.hide_index().background_gradient()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ce434_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ce434_row1_col1 {\n",
       "  background-color: #4897c4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ce434_row2_col1 {\n",
       "  background-color: #c9cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row3_col1 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row4_col1 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row5_col1 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row6_col1, #T_ce434_row7_col1 {\n",
       "  background-color: #ece7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row8_col1 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ce434_row9_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ce434\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ce434_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th id=\"T_ce434_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row0_col0\" class=\"data row0 col0\" > go</td>\n",
       "      <td id=\"T_ce434_row0_col1\" class=\"data row0 col1\" >0.059828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row1_col0\" class=\"data row1 col0\" > take</td>\n",
       "      <td id=\"T_ce434_row1_col1\" class=\"data row1 col1\" >0.043880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row2_col0\" class=\"data row2 col0\" > spend</td>\n",
       "      <td id=\"T_ce434_row2_col1\" class=\"data row2 col1\" >0.031570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row3_col0\" class=\"data row3 col0\" > make</td>\n",
       "      <td id=\"T_ce434_row3_col1\" class=\"data row3 col1\" >0.030519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row4_col0\" class=\"data row4 col0\" > do</td>\n",
       "      <td id=\"T_ce434_row4_col1\" class=\"data row4 col1\" >0.029206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row5_col0\" class=\"data row5 col0\" > be</td>\n",
       "      <td id=\"T_ce434_row5_col1\" class=\"data row5 col1\" >0.027960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row6_col0\" class=\"data row6 col0\" > attend</td>\n",
       "      <td id=\"T_ce434_row6_col1\" class=\"data row6 col1\" >0.025885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row7_col0\" class=\"data row7 col0\" > visit</td>\n",
       "      <td id=\"T_ce434_row7_col1\" class=\"data row7 col1\" >0.025827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row8_col0\" class=\"data row8 col0\" > run</td>\n",
       "      <td id=\"T_ce434_row8_col1\" class=\"data row8 col1\" >0.022074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ce434_row9_col0\" class=\"data row9 col0\" > have</td>\n",
       "      <td id=\"T_ce434_row9_col1\" class=\"data row9 col1\" >0.020955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbe3b2906a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_tokens = last_token_logits.topk(10)\n",
    "print(f\"most likely token index from topk is {most_likely_tokens.indices[0]}\") # this should be the same as argmax\n",
    "decoded_tokens = [tokenizer.decode(i) for i in most_likely_tokens.indices]\n",
    "probabilities_of_most_likely_tokens = last_token_probabilities[most_likely_tokens.indices]\n",
    "\n",
    "# Make a nice table to show the results\n",
    "most_likely_tokens_df = pd.DataFrame({\n",
    "    'tokens': decoded_tokens,\n",
    "    'probabilities': probabilities_of_most_likely_tokens,\n",
    "})\n",
    "# Show the table, in a nice formatted way (see https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#Builtin-Styles)\n",
    "# Caution: this \"gradient\" has *nothing* to do with gradient descent! (It's a color gradient.)\n",
    "most_likely_tokens_df.style.hide_index().background_gradient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that is given a phrase and a *k* and returns the top *k* most likely next tokens.\n",
    "\n",
    "Build this function using only code that you've already filled in above. Clean up the code so that it doesn't do or display anything extraneous. Add comments about what each step does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJWXqQLLkCyP",
    "outputId": "b969f2ed-2289-48c2-e717-b4802a493c5d"
   },
   "outputs": [],
   "source": [
    "def predict_next_tokens(phrase, k):\n",
    "    # Tokenize the phrase\n",
    "    input_ids = tokenizer.encode(phrase, return_tensors='pt')\n",
    "\n",
    "    # Generate the output tokens and probabilities\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        last_token_logits = output.logits[0, -1]\n",
    "        last_token_probabilities = torch.softmax(last_token_logits, dim=0)\n",
    "\n",
    "    # Get the top k most likely next tokens\n",
    "    topk_tokens = last_token_logits.topk(k)\n",
    "    decoded_tokens = [tokenizer.decode(token) for token in topk_tokens.indices]\n",
    "    probabilities = last_token_probabilities[topk_tokens.indices]\n",
    "\n",
    "    # Create a pandas dataframe to display the results\n",
    "    tokens_df = pd.DataFrame({\n",
    "        'tokens': decoded_tokens,\n",
    "        'probabilities': probabilities\n",
    "    })\n",
    "\n",
    "    return tokens_df\n",
    "\n",
    "\n",
    "\n",
    "check_global_vars(predict_next_tokens, allowed_globals=[\"torch\", \"tokenizer\", \"pd\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188717/1600815326.py:1: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  predict_next_tokens(\"This weekend I plan to\", 5).style.hide_index().background_gradient()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_49686_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49686_row1_col1 {\n",
       "  background-color: #7dacd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49686_row2_col1 {\n",
       "  background-color: #f4edf6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49686_row3_col1 {\n",
       "  background-color: #f9f2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49686_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_49686\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_49686_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th id=\"T_49686_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_49686_row0_col0\" class=\"data row0 col0\" > go</td>\n",
       "      <td id=\"T_49686_row0_col1\" class=\"data row0 col1\" >0.059828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_49686_row1_col0\" class=\"data row1 col0\" > take</td>\n",
       "      <td id=\"T_49686_row1_col1\" class=\"data row1 col1\" >0.043880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_49686_row2_col0\" class=\"data row2 col0\" > spend</td>\n",
       "      <td id=\"T_49686_row2_col1\" class=\"data row2 col1\" >0.031570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_49686_row3_col0\" class=\"data row3 col0\" > make</td>\n",
       "      <td id=\"T_49686_row3_col1\" class=\"data row3 col1\" >0.030519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_49686_row4_col0\" class=\"data row4 col0\" > do</td>\n",
       "      <td id=\"T_49686_row4_col1\" class=\"data row4 col1\" >0.029206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbe391340d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"This weekend I plan to\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188717/479713111.py:1: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  predict_next_tokens(\"To be or not to\", 5).style.hide_index().background_gradient()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5fcc3_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5fcc3_row1_col1 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5fcc3_row2_col1 {\n",
       "  background-color: #fef6fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5fcc3_row3_col1, #T_5fcc3_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5fcc3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5fcc3_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th id=\"T_5fcc3_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5fcc3_row0_col0\" class=\"data row0 col0\" > be</td>\n",
       "      <td id=\"T_5fcc3_row0_col1\" class=\"data row0 col1\" >0.648473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5fcc3_row1_col0\" class=\"data row1 col0\" > have</td>\n",
       "      <td id=\"T_5fcc3_row1_col1\" class=\"data row1 col1\" >0.021346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5fcc3_row2_col0\" class=\"data row2 col0\" > the</td>\n",
       "      <td id=\"T_5fcc3_row2_col1\" class=\"data row2 col1\" >0.012962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5fcc3_row3_col0\" class=\"data row3 col0\" > do</td>\n",
       "      <td id=\"T_5fcc3_row3_col1\" class=\"data row3 col1\" >0.009471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5fcc3_row4_col0\" class=\"data row4 col0\" >,</td>\n",
       "      <td id=\"T_5fcc3_row4_col1\" class=\"data row4 col1\" >0.007444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbdd873e4f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"To be or not to\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188717/1932537931.py:1: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  predict_next_tokens(\"For God so loved the\", 5).style.hide_index().background_gradient()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1ceaa_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1ceaa_row1_col1 {\n",
       "  background-color: #79abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1ceaa_row2_col1 {\n",
       "  background-color: #d0d1e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1ceaa_row3_col1 {\n",
       "  background-color: #e2dfee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1ceaa_row4_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1ceaa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1ceaa_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n",
       "      <th id=\"T_1ceaa_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1ceaa_row0_col0\" class=\"data row0 col0\" > world</td>\n",
       "      <td id=\"T_1ceaa_row0_col1\" class=\"data row0 col1\" >0.120081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ceaa_row1_col0\" class=\"data row1 col0\" > Lord</td>\n",
       "      <td id=\"T_1ceaa_row1_col1\" class=\"data row1 col1\" >0.072237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ceaa_row2_col0\" class=\"data row2 col0\" > people</td>\n",
       "      <td id=\"T_1ceaa_row2_col1\" class=\"data row2 col1\" >0.050226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ceaa_row3_col0\" class=\"data row3 col0\" > earth</td>\n",
       "      <td id=\"T_1ceaa_row3_col1\" class=\"data row3 col1\" >0.042750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ceaa_row4_col0\" class=\"data row4 col0\" > children</td>\n",
       "      <td id=\"T_1ceaa_row4_col1\" class=\"data row4 col1\" >0.026753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbdd8721e20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_tokens(\"For God so loved the\", 5).style.hide_index().background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsI_Tz0ipglx"
   },
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Explain the shape of `model_output.logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-32.2573, -30.3204, -32.3731,  ..., -43.4906, -43.1277, -32.2732],\n",
      "         [-58.8898, -62.1764, -65.4265,  ..., -69.8106, -67.6983, -63.7279],\n",
      "         [-74.3908, -75.3577, -77.8408,  ..., -84.0086, -74.2405, -76.7100],\n",
      "         [ -3.6331,  -5.7036,  -9.3678,  ..., -15.0274, -14.9595,  -7.1032],\n",
      "         [-77.8725, -79.7685, -82.1183,  ..., -88.5235, -86.5616, -79.6716]]])\n"
     ]
    }
   ],
   "source": [
    "print(model_output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of model_output.logits is a tensor with shape [batch_size, sequence_length, vocabulary_size]. The vocabulary_size dimension represents the number of possible tokens in the model's vocabulary, and each element in the tensor represents the model's logits (log-odds) for each possible token at each position in the sequence. Therefore, the shape of model_output.logits allows us to access the model's predictions for all tokens at all positions in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Change the -1 in `last_token_logits` to -2. What does the variable represent now? What does its argmax represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the -1 in last_token_logits to -2 refers to the logits of the second to last token in the sequence. This variable represents the model's predictions for the token that should come before the last token in the sequence. Taking its argmax will give us the index of the token that the model thinks is most likely to come before the last token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Let's think. The method in this notebook only get the scores for *one* next-token at a time. What if we wanted to do a whole sentence? We’d have to generate a token for each word in that sentence. What are a few different ways we could we adapt the approach used in this notebook to generate a complete sentence?\n",
    "\n",
    "To think about different ways to do this, think about what decision(s) you have to make when generating each token.\n",
    "\n",
    "Note: you don't have to write any code to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to adapt the approach used in this notebook to generate a complete sentence. One approach is to generate the next token based on the previously generated token and the entire context of the sentence generated so far. Another approach is to use beam search to generate multiple possible next tokens and choose the sequence of tokens with the highest overall probability. We could also use sampling to randomly generate the next token based on its probability distribution. Finally, we could use techniques such as temperature scaling to control the randomness of the generated text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVes22Uvu4sPkNN1/P8/pg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "013-lm-logits",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
