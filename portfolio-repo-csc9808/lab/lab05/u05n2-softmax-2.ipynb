{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax and Sigmoid\n",
    "\n",
    "Task: more practice using the `softmax` function, and connect it with the `sigmoid` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.softmax(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tensor([0.1, 0.2, 0.3])\n",
    "x2 = tensor([0.1, 0.2, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3006, 0.3322, 0.3672])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a chunk of code that assigns `p = softmax(x1)` then evaluates `p.sum()`. **Before you run it**, predict what the output will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it will be the sums of each items in the tensor above. so 1 when calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = softmax(x1)\n",
    "p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a chunk of code that evaluates `p2 = softmax(x2)` and displays the result. **Before you run it**, predict what it will output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By previewing the tensor value, we can see that first two values are way too small compared to the third tensor, so the value is going to be really close to 1, but not exactly 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0638e-44, 4.4842e-44, 1.0000e+00])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(softmax(x2))\n",
    "p2 = softmax(x2)\n",
    "p2.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate `torch.sigmoid(tensor(0.1))`. Write an expression that uses `softmax` to get the same output. *Hint*: Give `softmax` a two-element `tensor([num1, num2])`, where one of the numbers is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.sigmoid(tensor(0.1))=tensor(0.5250)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5250, 0.4750])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{torch.sigmoid(tensor(0.1))=}\")\n",
    "softmax(torch.sigmoid(tensor([0.405, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, softmax(x) will always have a valid probability as it transforms its input into probability distribution which sums up to 1. Function calculates the exponential of each element of x, and normalize the result by dividing each element by the sum of all element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Jargon alert: sometimes `x` is called the \"logits\" and `x.softmax(axis=0).log()` (or `x.log_softmax(axis=0)`) is called the \"logprobs\", short for \"log probabilities\". Complete the following expressions for `x1` (from the example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.2000, 0.3000])\n",
      "tensor([-1.2019, -1.1019, -1.0019])\n",
      "tensor([0.3006, 0.3322, 0.3672])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "logits = x1\n",
    "logprobs = x1.softmax(axis=0).log()\n",
    "probabilities = x1.softmax(axis=0)\n",
    "print(logits)\n",
    "print(logprobs)\n",
    "print(probabilities)\n",
    "print(probabilities.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In light of your observations about the difference between `softmax(x1)` and `softmax(x2)`, why might `softmax` be an appropriate name for this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the name softmax captures the function's key properties of converting a set of numbers to probabilities that sum to 1, while also highlighting its smoothing effect on the input values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
