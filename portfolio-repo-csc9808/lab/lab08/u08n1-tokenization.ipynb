{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc8Qlh1TEgC"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Task: Convert text to numbers; interpret subword tokenization.\n",
    "\n",
    "There are various different ways of converting text to numbers. This assignment works with one popular approach: assign numbers to parts of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_8RWp3TX-8"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUvTIxyWTdBF"
   },
   "source": [
    "We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n",
    "\n",
    "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` library is pre-installed on many systems, but in case you need to install it, you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWy--2nwhWPy",
    "outputId": "44b8e674-7e8b-4cf6-a1e9-1f8d62740382"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the transformers library\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "osKgPaDwhaN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNKbIh8hyDg"
   },
   "source": [
    "### Download and load the model\n",
    "\n",
    "This cell downloads the model and tokenizer, and loads them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IM5o_4w1hfyV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# We'll use this smaller version of GPT-2\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-Z9_U0LUEVQ",
    "outputId": "1d639faf-5b56-4bb2-81e5-054ee086ef0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 strings in its vocabulary.\n",
      "The model has 81,912,576 parameters.\n"
     ]
    }
   ],
   "source": [
    "token_to_id_dict = tokenizer.get_vocab()\n",
    "print(f\"The tokenizer has {len(token_to_id_dict)} strings in its vocabulary.\")\n",
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 tokens are: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
      "The last 10 tokens are: ['Ġ(/', 'âĢ¦.\"', 'Compar', 'Ġamplification', 'ominated', 'Ġregress', 'ĠCollider', 'Ġinformants', 'Ġgazed', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# warning: this assumes that there are no gaps in the token ids, which happens to be true for this tokenizer.\n",
    "id_to_token = [token for token, id in sorted(token_to_id_dict.items(), key=lambda x: x[1])]\n",
    "print(f\"The first 10 tokens are: {id_to_token[:10]}\")\n",
    "print(f\"The last 10 tokens are: {id_to_token[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOUiz_PsUZgS"
   },
   "source": [
    "## Task\n",
    "\n",
    "Consider the following phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JS7Z-DjoUiLK"
   },
   "outputs": [],
   "source": [
    "phrase = \"I visited Muskegon\"\n",
    "# Another one to try later. This was a famous early example of the GPT-2 model:\n",
    "# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting familiar with tokens\n",
    "\n",
    "1: Use `tokenizer.tokenize` to convert the phrase into a list of tokens. (What do you think the `Ġ` means?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyq-5XWSUx_8",
    "outputId": "22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠI', 'Ġvisited', 'ĠMus', 'ke', 'gon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(phrase)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I visited Muskegon\n"
     ]
    }
   ],
   "source": [
    "string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*.) Call the result `input_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkaoLSFMiHzb",
    "outputId": "18c6391e-a9aa-4d4c-dace-d49f8bbcba7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[314, 8672, 2629, 365, 14520]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(phrase, add_special_tokens=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Turn `input_ids` back into a readable string. Try this two ways: (1) using `convert_ids_to_tokens` and (2) using `tokenizer.decode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited Muskegon\n"
     ]
    }
   ],
   "source": [
    "# using convert_ids_to_tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "string1 = ''.join(tokens).replace('Ġ', ' ').strip()\n",
    "print(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ncSRaBaZix8R",
    "outputId": "204670f9-d7c4-4856-c804-a038b77ccd1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I visited Muskegon\n"
     ]
    }
   ],
   "source": [
    "# using tokenizer.decode\n",
    "string2 = tokenizer.decode(input_ids)\n",
    "print(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying what you learned\n",
    "\n",
    "5: Use `model.generate(tensor([input_ids]))` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input.) Call the result `output_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PZm3eIjjKCJ",
    "outputId": "1c4b1a63-de00-44f9-eee7-85714012dbc6"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(string)\n",
    "input_ids = torch.tensor([input_ids])  # add a batch dimension\n",
    "output_ids = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Convert your `output_ids` into a readable form. (Note: it has an extra \"batch\" dimension, so you'll need to use `output_ids[0]`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "2kKJ8rvijVez",
    "outputId": "386df167-0e88-45f1-b1a0-01beab1f0bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  314,  8672,  2629,   365, 14520,    11,   290,   314,   373,  6655,\n",
      "           284,  1064,   326,   262,  1748,   550,   407,   587,  1498,   284]])\n"
     ]
    }
   ],
   "source": [
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `generate` uses a greedy decoding by default, but it's highly customizable. We'll play more with it in later exercises. For now, if you want more interesting results, try:\n",
    "\n",
    "- Turn on `do_sample=True`. Run it a few times to see what it gives.\n",
    "- Set `top_k=5`. Or 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the largest possible token id for this tokenizer? What token does it correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Token corresponding to the largest possible id: 50256\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "max_token_id = vocab_size - 1\n",
    "print(\"Token corresponding to the largest possible id:\", max_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a brief explanation of what a tokenizer does. Note that we worked with two parts of a tokenizer in this exercise (one that deals only with strings, and another that deals with numbers); make sure your explanation addresses both parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer breaks down text into smaller units called tokens. It can convert a raw text string into a sequence of tokens, or a sequence of token ids that can be understood by a machine learning model. Tokens are building blocks for natural language processing, and can be individual words, subwords or characters. A tokenizer is essential in natural language processing for tasks such as language modeling, sentiment analysis, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What do you think the `Ġ` means? (Hint: it replaces a single well-known character.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ġ character is a special symbol used by some tokenization libraries, such as the Hugging Face Transformers library used in this exercise, to indicate the beginning of a new word or subword. The Ġ symbol is used instead of a space character, which is a well-known whitespace delimiter in natural language text.When a tokenizer encodes a text string and adds Ġ symbols, it essentially converts the original string into a sequence of subwords or characters that can be fed as input to a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Suppose you add some personal flair to your writing by doubling some letters. Explain what the tokenizer we have loaded up in this notebook will do with your embellished writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I double some letters in my writing, the tokenizer we have will treat them as separate subwords and assign a unique token ID to each one. This is because the tokenizer uses subword tokenization to break down words into smaller units based on their frequency of occurrence. The resulting sequence of token IDs is fed as input to a machine learning model, which predicts the most likely next word based on the learned patterns from the training data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNS7mRS03a7VSFcbdUnYf/k",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "012-tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
